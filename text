calendar function and finetune thw eights

cal
the dataset is in a single block so it will have to be split up

Use LoRA(Low rank adaptation) on the Bart model to target the attenjtion layers; can set rank value to determine the weighting
lora makes it easier to train the model on lightweight device(ESP32)
Seq2seq trainer to train over 3 epochs; loss decreased from 10 -> 1
script creates fake data and tests model
model tends to hallucinate


ft

